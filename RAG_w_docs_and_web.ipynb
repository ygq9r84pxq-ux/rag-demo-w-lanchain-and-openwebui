{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e95fd7-1471-4592-b7b8-0c5517475cae",
   "metadata": {},
   "source": [
    "# RAG with Local Docs and Web Search\n",
    "\n",
    "## Prerequisites & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a04d0b3-e1a9-443e-859d-6f924fdabb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (0.3.31)\n",
      "Requirement already satisfied: langchain-xai in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (0.2.5)\n",
      "Requirement already satisfied: psycopg2-binary in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (2.9.11)\n",
      "Requirement already satisfied: pgvector in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (0.4.2)\n",
      "Requirement already satisfied: pypdf in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (6.4.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain) (0.3.80)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain) (0.4.37)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain) (2.0.45)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
      "Requirement already satisfied: certifi in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: langchain-openai<0.4,>=0.3.28 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-xai) (0.3.35)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-openai<0.4,>=0.3.28->langchain-xai) (2.11.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-openai<0.4,>=0.3.28->langchain-xai) (0.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from openai<3.0.0,>=1.104.2->langchain-openai<0.4,>=0.3.28->langchain-xai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from openai<3.0.0,>=1.104.2->langchain-openai<0.4,>=0.3.28->langchain-xai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from openai<3.0.0,>=1.104.2->langchain-openai<0.4,>=0.3.28->langchain-xai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from openai<3.0.0,>=1.104.2->langchain-openai<0.4,>=0.3.28->langchain-xai) (4.67.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai<0.4,>=0.3.28->langchain-xai) (2025.11.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (9.5.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain-tavily in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (0.2.11)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-tavily) (3.13.2)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.20 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-tavily) (0.3.27)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-tavily) (0.3.80)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-tavily) (2.32.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.22.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (0.4.37)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (2.12.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (2.0.45)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (1.33)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (4.14.1)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<0.4.0,>=0.3.15->langchain-tavily) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.25.0)\n",
      "Requirement already satisfied: anyio in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (4.10.0)\n",
      "Requirement already satisfied: certifi in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (2.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain-postgres\n",
      "  Downloading langchain_postgres-0.0.16-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting asyncpg>=0.30.0 (from langchain-postgres)\n",
      "  Downloading asyncpg-0.31.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: langchain-core<2.0,>=0.2.13 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-postgres) (0.3.80)\n",
      "Requirement already satisfied: numpy<3,>=1.21 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-postgres) (2.0.2)\n",
      "Collecting pgvector<0.4,>=0.2.5 (from langchain-postgres)\n",
      "  Downloading pgvector-0.3.6-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting psycopg-pool<4,>=3.2.1 (from langchain-postgres)\n",
      "  Downloading psycopg_pool-3.2.8-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting psycopg<4,>=3 (from psycopg[binary]<4,>=3->langchain-postgres)\n",
      "  Downloading psycopg-3.2.13-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: sqlalchemy<3,>=2 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from sqlalchemy[asyncio]<3,>=2->langchain-postgres) (2.0.45)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-core<2.0,>=0.2.13->langchain-postgres) (0.4.37)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-core<2.0,>=0.2.13->langchain-postgres) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-core<2.0,>=0.2.13->langchain-postgres) (1.33)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-core<2.0,>=0.2.13->langchain-postgres) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-core<2.0,>=0.2.13->langchain-postgres) (4.14.1)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-core<2.0,>=0.2.13->langchain-postgres) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langchain-core<2.0,>=0.2.13->langchain-postgres) (2.12.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0,>=0.2.13->langchain-postgres) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=0.2.13->langchain-postgres) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=0.2.13->langchain-postgres) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=0.2.13->langchain-postgres) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=0.2.13->langchain-postgres) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=0.2.13->langchain-postgres) (0.25.0)\n",
      "Requirement already satisfied: anyio in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=0.2.13->langchain-postgres) (4.10.0)\n",
      "Requirement already satisfied: certifi in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=0.2.13->langchain-postgres) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=0.2.13->langchain-postgres) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=0.2.13->langchain-postgres) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=0.2.13->langchain-postgres) (0.16.0)\n",
      "Collecting psycopg-binary==3.2.13 (from psycopg[binary]<4,>=3->langchain-postgres)\n",
      "  Downloading psycopg_binary-3.2.13-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0,>=0.2.13->langchain-postgres) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0,>=0.2.13->langchain-postgres) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0,>=0.2.13->langchain-postgres) (0.4.2)\n",
      "Collecting greenlet>=1 (from sqlalchemy[asyncio]<3,>=2->langchain-postgres)\n",
      "  Downloading greenlet-3.2.4-cp39-cp39-macosx_11_0_universal2.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: async_timeout>=4.0.3 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from asyncpg>=0.30.0->langchain-postgres) (4.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=0.2.13->langchain-postgres) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=0.2.13->langchain-postgres) (2.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=0.2.13->langchain-postgres) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/emanuel/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=0.2.13->langchain-postgres) (1.3.1)\n",
      "Downloading langchain_postgres-0.0.16-py3-none-any.whl (46 kB)\n",
      "Downloading pgvector-0.3.6-py3-none-any.whl (24 kB)\n",
      "Downloading psycopg-3.2.13-py3-none-any.whl (206 kB)\n",
      "Downloading psycopg_pool-3.2.8-py3-none-any.whl (38 kB)\n",
      "Downloading psycopg_binary-3.2.13-cp39-cp39-macosx_11_0_arm64.whl (4.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading asyncpg-0.31.0-cp39-cp39-macosx_11_0_arm64.whl (639 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m639.9/639.9 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.2.4-cp39-cp39-macosx_11_0_universal2.whl (269 kB)\n",
      "Installing collected packages: psycopg-pool, psycopg-binary, psycopg, pgvector, greenlet, asyncpg, langchain-postgres\n",
      "\u001b[2K  Attempting uninstall: pgvector\n",
      "\u001b[2K    Found existing installation: pgvector 0.4.2\n",
      "\u001b[2K    Uninstalling pgvector-0.4.2:\n",
      "\u001b[2K      Successfully uninstalled pgvector-0.4.2\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7/7\u001b[0m [langchain-postgres]2m3/7\u001b[0m [pgvector]\n",
      "\u001b[1A\u001b[2KSuccessfully installed asyncpg-0.31.0 greenlet-3.2.4 langchain-postgres-0.0.16 pgvector-0.3.6 psycopg-3.2.13 psycopg-binary-3.2.13 psycopg-pool-3.2.8\n"
     ]
    }
   ],
   "source": [
    "!pip3 install langchain langchain-community langchain-xai psycopg2-binary pgvector pypdf\n",
    "!pip3 install sentence-transformers # huggingface ebedding/transformenr\n",
    "!pip3 install langchain-tavily\n",
    "!pip3 install langchain-postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0e377-cf93-4bd6-b676-f0740298fb2c",
   "metadata": {},
   "source": [
    "## The Implementation\n",
    "\n",
    "### Step 1: Configuration\n",
    "\n",
    "Set your API keys and database connection string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3f0474-7945-417d-abe7-534ce70f7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# API Keys\n",
    "os.environ[\"XAI_API_KEY\"] = \"xai-5l6...\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"tvly-dev-ho...\"\n",
    "\n",
    "# API key can also be loaded from .env file in same directory with this\n",
    "#from dotenv import load_dotenv\n",
    "#load_dotenv()\n",
    "#\n",
    "#openai_api_key = os.getenv(\"OPEN_API_KEY\")\n",
    "#tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "##\n",
    "# File format for .env:\n",
    "#OPENAI_API_KEY=xai-5l6...\n",
    "#TAVILY_API_KEY=tvly-dev-ho...\n",
    "\n",
    "# DB Connection (Update with your Postgres credentials)\n",
    "# Format: postgresql://user:password@localhost:5432/dbname\n",
    "CONNECTION_STRING = \"postgresql://postgres:password-here@192.168.10.10:5432/vector_db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd5b1a8-a58c-4c9f-aa60-4b63c538d74e",
   "metadata": {},
   "source": [
    "### Step 2: Document Ingestion\n",
    "\n",
    "This handles your PDFs and CSS files. We split them into chunks to fit the model's context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fd66d7b-994b-412b-ad64-d97b1e3aa428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Ignoring wrong pointing object 147 0 (offset 0)\n",
      "Ignoring wrong pointing object 153 0 (offset 0)\n",
      "Ignoring wrong pointing object 161 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 279 0 (offset 0)\n",
      "Ignoring wrong pointing object 283 0 (offset 0)\n",
      "Ignoring wrong pointing object 287 0 (offset 0)\n",
      "Ignoring wrong pointing object 324 0 (offset 0)\n",
      "Ignoring wrong pointing object 694 0 (offset 0)\n",
      "Ignoring wrong pointing object 698 0 (offset 0)\n",
      "Ignoring wrong pointing object 706 0 (offset 0)\n",
      "Ignoring wrong pointing object 708 0 (offset 0)\n",
      "Ignoring wrong pointing object 751 0 (offset 0)\n",
      "Ignoring wrong pointing object 754 0 (offset 0)\n",
      "Ignoring wrong pointing object 756 0 (offset 0)\n",
      "Ignoring wrong pointing object 760 0 (offset 0)\n",
      "Ignoring wrong pointing object 826 0 (offset 0)\n",
      "Ignoring wrong pointing object 912 0 (offset 0)\n",
      "Ignoring wrong pointing object 937 0 (offset 0)\n",
      "Ignoring wrong pointing object 941 0 (offset 0)\n",
      "Ignoring wrong pointing object 949 0 (offset 0)\n",
      "Ignoring wrong pointing object 987 0 (offset 0)\n",
      "Ignoring wrong pointing object 992 0 (offset 0)\n",
      "Ignoring wrong pointing object 994 0 (offset 0)\n",
      "Ignoring wrong pointing object 1021 0 (offset 0)\n",
      "Ignoring wrong pointing object 1246 0 (offset 0)\n",
      "Ignoring wrong pointing object 1248 0 (offset 0)\n",
      "Ignoring wrong pointing object 1250 0 (offset 0)\n",
      "Ignoring wrong pointing object 1310 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 110 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emanuel/Library/Python/3.9/lib/python/site-packages/langchain_community/vectorstores/pgvector.py:490: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  store = cls(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store Populated Successfully!\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import PGVector\n",
    "\n",
    "# DB CONFIGURATION\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASS = \"password-here\"\n",
    "DB_HOST = \"192.168.10.10\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"embeddings_db1\"\n",
    "\n",
    "# URL-encode the DB password to handle special characters like '@' or '#' safely\n",
    "encoded_pass = urllib.parse.quote_plus(DB_PASS)\n",
    "CONNECTION_STRING = f\"postgresql+psycopg2://{DB_USER}:{encoded_pass}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "# 1. Load Documents\n",
    "pdf_loader = DirectoryLoader('./docs/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "css_loader = DirectoryLoader('./docs/', glob=\"./*.css\", loader_cls=TextLoader)\n",
    "\n",
    "try:\n",
    "    docs = pdf_loader.load() + css_loader.load()\n",
    "    print(f\"Loaded {len(docs)} documents.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading docs: {e}\")\n",
    "    docs = []\n",
    "\n",
    "# 2. Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3. Create Embeddings & Store in Postgres\n",
    "# Note: You can ignore the \"LangChainDeprecationWarning\" for now; \n",
    "# it is just a warning, not an error.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "vectorstore = PGVector.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=splits,\n",
    "    collection_name=\"local_docs\",\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"Vector Store Populated Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f386ea-3270-4669-9cb0-82e64c4b176b",
   "metadata": {},
   "source": [
    "### Step 3: The \"Relevance Checker\" Logic\n",
    "\n",
    "We define a component that determines if the local documents actually answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6536c3c3-309d-42dc-91b9-56845fdf8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_xai import ChatXAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "llm = ChatXAI(model=\"grok-4-1-fast-non-reasoning\", temperature=0)\n",
    "\n",
    "# Define a grader schema\n",
    "class GradeRelevance(BaseModel):\n",
    "    binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeRelevance)\n",
    "\n",
    "system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \n",
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "relevance_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "])\n",
    "\n",
    "retrieval_grader = relevance_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9affd44-8e41-480c-9b21-d5ce7591e473",
   "metadata": {},
   "source": [
    "### Step 4: The Fallback RAG Chain\n",
    "\n",
    "This is the core logic: Search local -> Check Relevance -> If fail, use Tavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63236ade-e151-4310-99db-b0997dc089fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Web...\n",
      "Bot: **Your last question about Emanuel was:**  \n",
      "*\"how would you rate emanuels experties with networking sollution and design?\"*  \n",
      "\n",
      "(I rated it **8.5/10**‚Äîadvanced/expert level in SASE, QoS, ThousandEyes, and global SaaS infra. Spot on from our chat history! üòé)\n",
      "Searching Web...\n",
      "Bot: **Emanuel!** üé§  \n",
      "\n",
      "(As you've told me directly in our chat‚Äîstraight from the \"Ooh Na Na\" Rihanna vibes. Even with all those quiz transcripts and Google Assistant echoes, I've got it locked in from history. No guessing needed! üòé)\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import psycopg\n",
    "import urllib.parse\n",
    "from langchain_postgres import PostgresChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "# --- 1. Connection & Helper ---\n",
    "# Ensure the connection string uses 'psycopg' (v3) for this specific component\n",
    "# We strip the '+psycopg2' if it exists\n",
    "history_conn_string = CONNECTION_STRING.replace(\"+psycopg2\", \"\")\n",
    "sync_connection = psycopg.connect(history_conn_string) \n",
    "table_name = \"chat_history\"\n",
    "\n",
    "# Helper to make sure our session_id is a valid UUID\n",
    "def ensure_uuid(id_str: str) -> str:\n",
    "    try:\n",
    "        # Check if it's already a valid UUID\n",
    "        return str(uuid.UUID(id_str))\n",
    "    except ValueError:\n",
    "        # If not, create a deterministic UUID based on your string\n",
    "        # This ensures \"user_123\" always results in the same UUID\n",
    "        return str(uuid.uuid5(uuid.NAMESPACE_DNS, id_str))\n",
    "\n",
    "# Create tables if it doesnt aleready exists\n",
    "PostgresChatMessageHistory.create_tables(sync_connection, table_name)\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    valid_id = ensure_uuid(session_id)\n",
    "    return PostgresChatMessageHistory(\n",
    "        table_name,\n",
    "        valid_id,\n",
    "        sync_connection=sync_connection\n",
    "    )\n",
    "\n",
    "# --- 2. Updated RAG Logic ---\n",
    "def run_rag_with_history(question, session_id=\"default_user\"):\n",
    "    # Normalize the session ID to UUID\n",
    "    valid_session_id = ensure_uuid(session_id)\n",
    "    \n",
    "    # Fetch history\n",
    "    history_manager = get_session_history(valid_session_id)\n",
    "    history_messages = history_manager.messages\n",
    "    \n",
    "    # 1. Retrieve & Grade Local Docs\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\"\n",
    "    for d in docs:\n",
    "        grade = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        if grade.binary_score.lower() == \"yes\":\n",
    "            context += d.page_content + \"\\n\"\n",
    "    \n",
    "    # 2. Web Fallback\n",
    "    source = \"Local Knowledge Base\"\n",
    "    if not context.strip():\n",
    "        print(\"Searching Web...\")\n",
    "        source = \"Tavily Web Search\"\n",
    "        web_results = web_search_tool.invoke({\"query\": question})\n",
    "        context = \"\\n\".join([res.get(\"content\", \"\") for res in web_results])\n",
    "\n",
    "    # 3. Final Chain\n",
    "    qa_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant. Use context and history to answer.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{question}\")\n",
    "    ])\n",
    "    \n",
    "    rag_chain = qa_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    response = rag_chain.invoke({\n",
    "        \"question\": question, \n",
    "        \"context\": context,\n",
    "        \"chat_history\": history_messages\n",
    "    })\n",
    "    \n",
    "    # 4. Save to Postgres\n",
    "    history_manager.add_messages([\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response)\n",
    "    ])\n",
    "    \n",
    "    return {\"answer\": response, \"source\": source}\n",
    "\n",
    "# --- Test ---\n",
    "session_name = \"my_custom_session\" # The helper will convert this to a UUID\n",
    "res = run_rag_with_history(\"what was my last question about emanuel?.\", session_name)\n",
    "print(f\"Bot: {res['answer']}\")\n",
    "\n",
    "res2 = run_rag_with_history(\"What is my name?\", session_name)\n",
    "print(f\"Bot: {res2['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b1963-cb3a-4187-9bde-bde0b694a72b",
   "metadata": {},
   "source": [
    "#### This is test the summary rotation feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16476569-a16d-4818-8675-7fcf678bccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Turn 1 ---\n",
      "‚ö†Ô∏è History is long (8 msgs). Summarizing to save memory...\n",
      "‚úÖ History compressed. New Context: The user shared three key personal details: their ...\n",
      "\n",
      "--- Turn 2 ---\n",
      "\n",
      "--- Turn 3 ---\n",
      "\n",
      "--- Turn 4 ---\n",
      "‚ö†Ô∏è History is long (7 msgs). Summarizing to save memory...\n",
      "‚úÖ History compressed. New Context: **Updated Conversation Summary:** The user has rec...\n",
      "\n",
      "--- Turn 5 ---\n",
      "\n",
      "--- Turn 6 ---\n",
      "\n",
      "--- Turn 7 ---\n",
      "‚ö†Ô∏è History is long (7 msgs). Summarizing to save memory...\n",
      "‚úÖ History compressed. New Context: **Updated Conversation Summary:** User reconfirmed...\n",
      "\n",
      "Final Answer: **Your favorite color is blue, and you live in New York.**\n",
      "\n",
      "This is confirmed from our previous quiz sequence where you reconfirmed these core details (along with driving a Honda Civic). Blue ties into your NYC vibe, gym sessions in blue Nike gear, and even blue burger bun hacks‚Äîkeeping that consistent thread alive! üíôüåÉ What's next‚Äîgym routine deets, exact NYC neighborhood, or a blue-themed burger spot rec?\n"
     ]
    }
   ],
   "source": [
    "test_session = \"summary_test_session\"\n",
    "\n",
    "# Turn 1\n",
    "print(\"--- Turn 1 ---\")\n",
    "run_rag_with_summary(\"My favorite color is Blue.\", test_session)\n",
    "\n",
    "# Turn 2\n",
    "print(\"\\n--- Turn 2 ---\")\n",
    "run_rag_with_summary(\"I live in New York.\", test_session)\n",
    "\n",
    "# Turn 3\n",
    "print(\"\\n--- Turn 3 ---\")\n",
    "run_rag_with_summary(\"I drive a Honda Civic.\", test_session)\n",
    "\n",
    "# Turn 4\n",
    "print(\"\\n--- Turn 4 ---\")\n",
    "run_rag_with_summary(\"I love burgers.\", test_session)\n",
    "\n",
    "# Turn 5\n",
    "print(\"\\n--- Turn 5 ---\")\n",
    "run_rag_with_summary(\"I go to the gym.\", test_session)\n",
    "\n",
    "# Turn 6\n",
    "print(\"\\n--- Turn 6 ---\")\n",
    "run_rag_with_summary(\"I work at the nike store.\", test_session)\n",
    "\n",
    "# Turn 7 (This should trigger the check next time because 3 turns = 6 messages)\n",
    "print(\"\\n--- Turn 7 ---\")\n",
    "res = run_rag_with_summary(\"What is my favorite color and where do I live?\", test_session)\n",
    "print(f\"\\nFinal Answer: {res['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd13daba-db9e-4903-8260-5e445f1ba857",
   "metadata": {},
   "source": [
    "#### This is section 4 but in stead of simple endless history it employs history summarization after X history messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4db4773-90f0-460b-8617-871ece838b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ### **TriageLogic Information Security Policy** (Policy #15, Effective 11/13/13) üîí\n",
      "\n",
      "**Applies To**: ‚òëÔ∏è TriageLogic ‚òëÔ∏è PRN ‚òëÔ∏è Phone RN\n",
      "\n",
      "#### **Core Controls** (Direct from III. Information Confidentiality and Security CORE 15):\n",
      "| **Control** | **Details** |\n",
      "|-------------|-------------|\n",
      "| **Intrusion Protection** | Controls to prevent **malicious hackers** and **unauthorized access**. |\n",
      "| **Traffic Monitoring** | **Firewall** (or equivalent) to examine **inbound/outbound network activity**. |\n",
      "| **Risk Management** | - **Risk assessments**<br>- **Data handling policies**<br>- **Data Loss Prevention (DLP)**<br>- **Record all security policies/procedures** |\n",
      "\n",
      "#### **Email/Communication Policy**:\n",
      "- **Email is NOT secure**‚Äî**NO PHI** (Protected Health Information) allowed in emails or other forms of communication.\n",
      "\n",
      "**Summary**: **HIPAA-Aligned Perimeter Security**‚Äîfirewall, intrusion prevention, DLP, risk mgmt + strict no-PHI email rule.\n",
      "\n",
      "**Your Fit (Emanuel, 8.5/10 Networking)**: **Firewall/traffic monitoring** = your **SASE/ThousandEyes** expertise. üòé\n",
      "Searching Web...\n",
      "Bot: **Your name is Emanuel!** üé§  \n",
      "\n",
      "(From our chat history: You told me directly‚Äî\"My name is Emanuel\"‚Äîand we've looped on it with Rihanna's \"Ooh Na Na What's My Name,\" your **8.5/10 networking expertise**, **core skills** in IT Management/Cloud Ops/Architecture, **TriageLogic BCDR Policy #15** (30-min vendor SLA), **InfoSec Policy** (firewall/DLP/no PHI email), favorite color teases, and endless wikiHow/Google Assistant \"What is my name?\" quizzes. Sonorant name vibes (agreeable/conscientious)‚Äî**locked in**! üòé)\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import psycopg\n",
    "import urllib.parse\n",
    "from langchain_postgres import PostgresChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, RemoveMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "# --- 1. Configuration & Prompts ---\n",
    "\n",
    "# This prompt tells the LLM how to compress the history\n",
    "summarizer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Summarize the following conversation history into a concise paragraph. \"\n",
    "               \"Include key facts, user preferences, and any unresolved questions. \"\n",
    "               \"Do not lose important details.\"),\n",
    "    (\"placeholder\", \"{chat_history}\")\n",
    "])\n",
    "\n",
    "# Helper to manage UUIDs (from our previous fix)\n",
    "def ensure_uuid(id_str: str) -> str:\n",
    "    try:\n",
    "        return str(uuid.UUID(id_str))\n",
    "    except ValueError:\n",
    "        return str(uuid.uuid5(uuid.NAMESPACE_DNS, id_str))\n",
    "\n",
    "# --- 2. The Summarization Logic ---\n",
    "\n",
    "def manage_conversation_history(session_id):\n",
    "    \"\"\"\n",
    "    Checks history length. If > 6 messages, summarizes them to save tokens.\n",
    "    \"\"\"\n",
    "    # Get the history manager\n",
    "    history_manager = get_session_history(session_id)\n",
    "    messages = history_manager.messages\n",
    "    \n",
    "    # Threshold: If we have more than 6 messages (3 turns), we summarize\n",
    "    if len(messages) > 6:\n",
    "        print(f\"‚ö†Ô∏è History is long ({len(messages)} msgs). Summarizing to save memory...\")\n",
    "        \n",
    "        # A. Generate Summary using xAI\n",
    "        summarizer_chain = summarizer_prompt | llm | StrOutputParser()\n",
    "        summary_text = summarizer_chain.invoke({\"chat_history\": messages})\n",
    "        \n",
    "        # B. Clear the database storage for this session\n",
    "        history_manager.clear()\n",
    "        \n",
    "        # C. Add the Summary as a 'SystemMessage' so the bot knows what happened\n",
    "        # We also keep the last 2 messages (context) if you prefer, but \n",
    "        # for simplicity, we start fresh with just the summary here.\n",
    "        history_manager.add_message(\n",
    "            SystemMessage(content=f\"PREVIOUS CONVERSATION SUMMARY: {summary_text}\")\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ History compressed. New Context: {summary_text[:50]}...\")\n",
    "\n",
    "# --- 3. The Optimized RAG Pipeline ---\n",
    "\n",
    "def run_rag_with_summary(question, session_id=\"user_default\"):\n",
    "    # 1. Prepare Session\n",
    "    valid_id = ensure_uuid(session_id)\n",
    "    \n",
    "    # 2. OPTIMIZATION: Check & Summarize History BEFORE processing\n",
    "    manage_conversation_history(valid_id)\n",
    "    \n",
    "    # 3. Retrieve & Grade Local Docs\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\"\n",
    "    for d in docs:\n",
    "        grade = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        if grade.binary_score.lower() == \"yes\":\n",
    "            context += d.page_content + \"\\n\"\n",
    "    \n",
    "    # 4. Web Fallback\n",
    "    source = \"Local Knowledge Base\"\n",
    "    if not context.strip():\n",
    "        source = \"Tavily Web Search\"\n",
    "        # Note: Using the web_search_tool we initialized earlier\n",
    "        web_results = web_search_tool.invoke({\"query\": question})\n",
    "        if isinstance(web_results, list):\n",
    "            context = \"\\n\".join([res.get(\"content\", \"\") for res in web_results])\n",
    "        else:\n",
    "            context = str(web_results)\n",
    "\n",
    "    # 5. Generate Answer\n",
    "    # We fetch the (possibly summarized) history now\n",
    "    history_manager = get_session_history(valid_id)\n",
    "    current_history = history_manager.messages\n",
    "    \n",
    "    qa_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant for TriageLogic. \"\n",
    "                   \"Answer the user's question using ONLY the provided context below. \"\n",
    "                   \"The Chat History is for context reference only; do not mix unrelated past topics into the answer.\"\n",
    "                   \"If the answer is not in the context, say you don't know.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"), # This now contains the Summary + recent msgs\n",
    "        (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{question}\")\n",
    "    ])\n",
    "    \n",
    "    rag_chain = qa_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    response = rag_chain.invoke({\n",
    "        \"question\": question, \n",
    "        \"context\": context,\n",
    "        \"chat_history\": current_history\n",
    "    })\n",
    "    \n",
    "    # 6. Save this new turn\n",
    "    history_manager.add_messages([\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response)\n",
    "    ])\n",
    "    \n",
    "    return {\"answer\": response, \"source\": source}\n",
    "\n",
    "# --- Test ---\n",
    "session_name = \"my_custom_session\" # The helper will convert this to a UUID\n",
    "res = run_rag_with_history(\"whats the information security policy?.\", session_name)\n",
    "print(f\"Bot: {res['answer']}\")\n",
    "\n",
    "res2 = run_rag_with_history(\"What is my name?\", session_name)\n",
    "print(f\"Bot: {res2['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f761f-d9cc-4d9d-8b7d-ae7bd57c595d",
   "metadata": {},
   "source": [
    "#### Reset session IDs for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4258801-38d7-4ec5-963a-80275870c2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ History for session 'summary_test_session' has been WIPED.\n",
      "Session 'user_default' was already empty.\n"
     ]
    }
   ],
   "source": [
    "def clear_session_history(session_id):\n",
    "    \"\"\"\n",
    "    Manually wipes the conversation history for a specific session.\n",
    "    Useful for restarting tests.\n",
    "    \"\"\"\n",
    "    valid_id = ensure_uuid(session_id)\n",
    "    history_manager = get_session_history(valid_id)\n",
    "    \n",
    "    # Check if there are messages to delete\n",
    "    if len(history_manager.messages) > 0:\n",
    "        history_manager.clear()\n",
    "        print(f\"üßπ History for session '{session_id}' has been WIPED.\")\n",
    "    else:\n",
    "        print(f\"Session '{session_id}' was already empty.\")\n",
    "\n",
    "# --- Run this once to fix your current situation ---\n",
    "# Replace 'summary_test_session' with whatever ID you were using\n",
    "clear_session_history(\"summary_test_session\") \n",
    "clear_session_history(\"user_default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c25bc4-a9a5-45e8-b068-5b4a00901c27",
   "metadata": {},
   "source": [
    "#### Testing answer quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7dfdd432-c1ee-4122-acb7-a4bbba573b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ANSWER ---\n",
      "TriageLogic's Business Continuity / Disaster Recovery policy (Policy #15, effective 11/13/13, last reviewed 9/27/16) applies to Triage Logic and PRN Phone RN. It requires contracting only with software vendors that meet minimum Business Continuity Plan requirements (CORE14 a, b, c, d). Specifically, the vendor must provide 24/7 emergency support for software or server issues (365 days/year) with a 30-minute response time for emergencies.\n",
      "\n",
      "--- RAW SOURCE CONTENT ---\n",
      "\n",
      "[Chunk 0] Content:\n",
      "58\t\n",
      "II. Business Continuity / Disaster Recovery  This\tpolicy/procedure\tapplies\tto:\t _X_Triage\tLogic\t _X_PRN\t \t\t\t\t\tPhone\tRN\t\n",
      "Effective\tDate:\t11/13/13...\n",
      "\n",
      "[Chunk 1] Content:\n",
      "Name\tof\tPolicy/Procedure:\tBusiness\tContinuity\t Most\tRecent\tRevision\tDate:\tPolicy\t#:\t15\t Most\tRecent\tReview:\t9/27/16\t  A. Business Continuity Plan CORE14\ta,\tb,\tc\t,d\t\t TriageLogic will only contract with a software vendor that provides the following minimum requirements  B. Support Infrastructure: The...\n",
      "\n",
      "[Chunk 2] Content:\n",
      "Below describes the different scenarios that can occur and how the operations are to continue. An email sent to 911@triagelogic.com will alert all the on call IT staff as well as the Medical Director and COO of an IT emergency.  1. Primary server is unavailable ‚Äì Nurse Manager verifies that more tha...\n"
     ]
    }
   ],
   "source": [
    "# 1. Start a fresh chat (clears the 'skills' noise)\n",
    "result = run_rag_with_summary(\"What is the company policy for disaster recovery?\", session_id=\"test_policy_1\")\n",
    "\n",
    "# 2. Print the answer\n",
    "print(\"\\n--- ANSWER ---\")\n",
    "print(result['answer'])\n",
    "\n",
    "# 3. CRITICAL: See exactly what the bot read\n",
    "# This will show you the raw text retrieved from the PDF. \n",
    "# If this text is missing the specific detail, we need to adjust chunk_overlap.\n",
    "docs = retriever.invoke(\"company policy for disaster recovery\")\n",
    "print(\"\\n--- RAW SOURCE CONTENT ---\")\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"\\n[Chunk {i}] Content:\\n{d.page_content[:300]}...\") # Printing first 300 chars of each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19599a22-3b2b-4a68-9d66-ca7480b0bca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
